{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings and Semantic Analysis\n",
    "\n",
    "This notebook explores various techniques for analyzing word relationships, similarities, and semantic meanings using WordNet and Word2Vec models.\n",
    "\n",
    "## WordNet-based Analysis\n",
    "\n",
    "First, we'll use WordNet, a lexical database for the English language, to explore word taxonomies and similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "all_hypernyms = list(set(wn.all_synsets(pos='n')))\n",
    "\n",
    "# Print the taxonomy (hypernym hierarchy) for a given word. Default is noun 'n'\n",
    "def print_taxonomy(word, pos='n'):\n",
    "    synset = wn.synsets(word, pos=pos)[0]\n",
    "    hyper = lambda s: s.hypernyms()\n",
    "    taxonomy = list(synset.closure(hyper))\n",
    "    \n",
    "    print(f\"Taxonomy for '{word}':\")\n",
    "    # Print each hypernym in reverse order (from most general to most specific)\n",
    "    for i, hypernym in enumerate(reversed(taxonomy), 1):\n",
    "        print(f\"{i}. {hypernym.name().split('.')[0]} ({hypernym.definition()})\")\n",
    "    print(f\"{len(taxonomy) + 1}. {synset.name().split('.')[0]} ({synset.definition()})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the taxonomy for \"lion\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_taxonomy(\"lion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity using WordNet\n",
    "Now, we'll define functions to calculate cosine similarity between words based on their WordNet hypernym vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    synset = wn.synsets(word)[0]\n",
    "    hypernyms = list(synset.closure(lambda s: s.hypernyms()))\n",
    "    return [1 if h in hypernyms else 0 for h in all_hypernyms]\n",
    "\n",
    "# Compute dot product of the two vectors divided by the product of their norms\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def show_similarity(word1, word2):\n",
    "    vec1 = get_vector(word1)\n",
    "    vec2 = get_vector(word2)\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_similarity(\"lion\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_similarity(\"lion\", \"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_similarity(\"lion\", \"paper\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model Analysis\n",
    "\n",
    "Now, we'll switch to using a pre-trained Word2Vec model for more advanced word relationship analysis.\n",
    "\n",
    "### Loading the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Download and load the pre-trained Word2Vec model\n",
    "print(\"Downloading and loading the model... This may take a few minutes.\")\n",
    "model = api.load('word2vec-google-news-300')\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Word Vectors\n",
    "Let's look at the raw vector for \"dog\" and the vocabulary size of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the word vector regardless of case\n",
    "def get_word_vector(word):\n",
    "    if word.lower() in model.key_to_index:\n",
    "        return model[word.lower()]\n",
    "    elif word.upper() in model.key_to_index:\n",
    "        return model[word.upper()]\n",
    "    elif word.capitalize() in model.key_to_index:\n",
    "        return model[word.capitalize()]\n",
    "    else:\n",
    "        raise KeyError(f\"Word '{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print raw vector of a word\n",
    "print(\"\\nRaw vector for 'dog':\")\n",
    "print(get_word_vector(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Print vocabulary size\n",
    "print(\"\\nVocabulary size:\", len(model.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similar Words\n",
    "We'll define functions to find words similar to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a word is a valid English word and lemmatize it\n",
    "def process_word(word):\n",
    "    word = word.lower()\n",
    "    if word.isalpha() and not word.startswith('_') and not word.endswith('_') and '_' not in word:\n",
    "        return lemmatizer.lemmatize(word)\n",
    "    return None\n",
    "\n",
    "# Function to check if a word is in the vocabulary\n",
    "def word_in_vocab(word):\n",
    "    return word.lower() in model.key_to_index or word.upper() in model.key_to_index or word.capitalize() in model.key_to_index\n",
    "\n",
    "# Function to find similar words\n",
    "def find_similar_words(word, topn=5):\n",
    "    if not word_in_vocab(word):\n",
    "        print(f\"'{word}' is not in the vocabulary.\")\n",
    "        return\n",
    "    print(f\"\\nWords similar to '{word}':\")\n",
    "    original_word = process_word(word)\n",
    "    similar_words = model.most_similar(positive=[get_word_vector(word)], topn=50)  # Get more words than needed\n",
    "    valid_words = []\n",
    "    for similar_word, score in similar_words:\n",
    "        processed_word = process_word(similar_word)\n",
    "        if processed_word and processed_word != original_word and processed_word not in [w for w, _ in valid_words]:\n",
    "            valid_words.append((processed_word, score))\n",
    "            if len(valid_words) == topn:\n",
    "                break\n",
    "    \n",
    "    # If we don't have enough words, relax the filtering\n",
    "    if len(valid_words) < topn:\n",
    "        for similar_word, score in similar_words:\n",
    "            if similar_word not in [w for w, _ in valid_words]:\n",
    "                valid_words.append((similar_word, score))\n",
    "                if len(valid_words) == topn:\n",
    "                    break\n",
    "    \n",
    "    for similar_word, score in valid_words[:topn]:\n",
    "        print(f\"{similar_word}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find words similar to \"dog\", \"computer\", and \"president\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Demonstrate similarity\n",
    "find_similar_words(\"dog\")\n",
    "find_similar_words(\"computer\")\n",
    "find_similar_words(\"president\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Least Similar Words\n",
    "Now, let's find words that are least similar to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import words as nltk_words\n",
    "import nltk\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "def find_least_similar_words(word, topn=5, min_count=1000):\n",
    "    if word not in model:\n",
    "        print(f\"'{word}' is not in the vocabulary.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nWords least similar to '{word}':\")\n",
    "    \n",
    "    # Get common English words\n",
    "    common_words = set(w.lower() for w in nltk_words.words())\n",
    "    \n",
    "    # Filter vocabulary to only include common words and words that appear frequently\n",
    "    filtered_vocab = [w for w in model.key_to_index if w in common_words and model.get_vecattr(w, 'count') >= min_count]\n",
    "    \n",
    "    # If we have too few words, reduce the min_count\n",
    "    while len(filtered_vocab) < 1000 and min_count > 100:\n",
    "        min_count //= 2\n",
    "        filtered_vocab = [w for w in model.key_to_index if w in common_words and model.get_vecattr(w, 'count') >= min_count]\n",
    "    \n",
    "    # Sample words from the filtered vocabulary\n",
    "    sample_size = min(10000, len(filtered_vocab))\n",
    "    sampled_words = random.sample(filtered_vocab, sample_size)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = [(w, model.similarity(word, w)) for w in sampled_words if w != word]\n",
    "    \n",
    "    # Sort by similarity (ascending) and get the least similar words\n",
    "    least_similar = sorted(similarities, key=lambda x: x[1])[:topn]\n",
    "    \n",
    "    for similar_word, score in least_similar:\n",
    "        print(f\"{similar_word}: {score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find words least similar to \"dog\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_least_similar_words(\"dog\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analogies\n",
    "We'll explore word analogies using the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform word analogy\n",
    "def word_analogy(word1, word2, word3):\n",
    "    if not all(word_in_vocab(w) for w in [word1, word2, word3]):\n",
    "        print(\"One or more words are not in the vocabulary.\")\n",
    "        return\n",
    "    results = model.most_similar(positive=[get_word_vector(word2), get_word_vector(word3)], \n",
    "                                 negative=[get_word_vector(word1)], topn=10)\n",
    "    valid_results = []\n",
    "    for result_word, score in results:\n",
    "        processed_word = process_word(result_word)\n",
    "        if processed_word and processed_word not in [word1, word2, word3]:\n",
    "            valid_results.append((processed_word, score))\n",
    "            if len(valid_results) == 1:\n",
    "                break\n",
    "    if valid_results:\n",
    "        result = valid_results[0]\n",
    "        print(f\"\\nAnalogy: {word1} is to {word2} as {word3} is to {result[0]}\")\n",
    "    else:\n",
    "        print(\"No valid analogy found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some analogies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Demonstrate analogy\n",
    "word_analogy(\"man\", \"king\", \"woman\")\n",
    "word_analogy(\"paris\", \"france\", \"rome\")\n",
    "word_analogy(\"good\", \"best\", \"bad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Word Embeddings\n",
    "Finally, let's visualize word embeddings in a 2D space using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize word embeddings\n",
    "def visualize_embeddings(words):\n",
    "    valid_words = [word for word in words if word_in_vocab(word) and process_word(word)]\n",
    "    if not valid_words:\n",
    "        print(\"None of the provided words are valid words in the vocabulary.\")\n",
    "        return\n",
    "    word_vectors = [get_word_vector(word) for word in valid_words]\n",
    "    \n",
    "    # Convert list of vectors to a numpy array\n",
    "    word_vectors_array = np.array(word_vectors)\n",
    "    \n",
    "    # Adjust perplexity based on the number of samples\n",
    "    n_samples = word_vectors_array.shape[0]\n",
    "    perplexity = min(30, n_samples - 1)  # Default is 30, but we need it to be less than n_samples\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=perplexity)\n",
    "    vectors_2d = tsne.fit_transform(word_vectors_array)\n",
    "    \n",
    "    plt.figure(figsize=(16, 14))\n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1])\n",
    "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))\n",
    "    plt.title(\"Word Embedding Visualization\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a set of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize embeddings\n",
    "words_to_plot = [\n",
    "    \"love\", \"hate\", \"joy\", \"sadness\",\n",
    "    \"computer\", \"internet\", \"technology\", \"science\",\n",
    "    \"democracy\", \"freedom\", \"justice\", \"equality\",\n",
    "    \"earth\", \"moon\", \"sun\", \"galaxy\",\n",
    "    \"music\", \"art\", \"dance\", \"literature\",\n",
    "    \"health\", \"disease\", \"medicine\", \"wellness\",\n",
    "    \"money\", \"wealth\", \"poverty\", \"economy\",\n",
    "    \"war\", \"peace\", \"conflict\", \"diplomacy\",\n",
    "    \"education\", \"knowledge\", \"learning\", \"wisdom\",\n",
    "    \"time\", \"space\", \"reality\", \"imagination\"\n",
    "]\n",
    "visualize_embeddings(words_to_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
